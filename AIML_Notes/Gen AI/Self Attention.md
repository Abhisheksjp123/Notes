### Origin of Self attention
The primary goal of Natural Language Processing (NLP) is to enable machines to understand human language. The fundamental step in this process is converting words into numbers that a computer can process. Over the years, several techniques were developed to achieve this.
## 1. One-Hot Encoding (OHE)
One of the earliest techniques was **One-Hot Encoding**. This method creates a vector for each word with a length equal to the size of the entire vocabulary. The vector is all zeros except for a single "1" at the index corresponding to that specific word.
For a vocabulary of `mat`, `cat`, `rat`, the representations would be:

|Word|mat|cat|rat|
|---|---|---|---|
|**mat**|1|0|0|
|**cat**|0|1|0|
|**rat**|0|0|1|
A sentence like "mat cat rat" would be represented as `[[1,0,0], [0,1,0], [0,0,1]]`.
## 2. Bag-of-Words (BoW)

The **Bag-of-Words** model improved on this by representing an entire sentence or document as a single vector. Instead of just a "1," the vector would contain the **count** of each word's occurrences.
For the sentence "mat rat mat":

|Word|mat|cat|rat|
|---|---|---|---|
|**Count**|2|0|1|and there were several other techniques like tf-idf
The sentence is represented as the vector `[2, 0, 1]`. Variations like **TF-IDF** (Term Frequency-Inverse Document Frequency) were also developed to weigh words by their importance.

- **Problem**: While BoW captures word frequency, it completely ignores word order and context. "The cat chased the rat" and "The rat chased the cat" would have identical representations, even though their meanings are opposite.

Excellent, you're starting at the right place. To understand self-attention, you first need to understand the evolution of how we represent words numerically.

Here are your notes, refined for clarity.

The Journey to Understanding Language

The primary goal of Natural Language Processing (NLP) is to enable machines to understand human language. The fundamental step in this process is converting words into numbers that a computer can process. Over the years, several techniques were developed to achieve this.

1. One-Hot Encoding (OHE)

One of the earliest techniques was One-Hot Encoding. This method creates a vector for each word with a length equal to the size of the entire vocabulary. The vector is all zeros except for a single "1" at the index corresponding to that specific word.

For a vocabulary of mat, cat, rat, the representations would be:
Word	mat	cat	rat
mat	1	0	0
cat	0	1	0
rat	0	0	1

A sentence like "mat cat rat" would be represented as [[1,0,0], [0,1,0], [0,0,1]].

Problem: This method fails to capture any relationship between words. The vectors for "cat" and "rat" are just as different as the vectors for "cat" and "mat." It also creates very large and sparse (mostly empty) vectors for large vocabularies.

2. Bag-of-Words (BoW)

The Bag-of-Words model improved on this by representing an entire sentence or document as a single vector. Instead of just a "1," the vector would contain the count of each word's occurrences.

For the sentence "mat rat mat":
Word	mat	cat	rat
Count	2	0	1

The sentence is represented as the vector [2, 0, 1]. Variations like TF-IDF (Term Frequency-Inverse Document Frequency) were also developed to weigh words by their importance.

Problem: While BoW captures word frequency, it completely ignores word order and context. "The cat chased the rat" and "The rat chased the cat" would have identical representations, even though their meanings are opposite.

These early methods were a crucial first step, but they couldn't capture the rich semantic meaning of words. The true breakthrough, as you noted, came with the introduction of ==Word Embeddings==.
## 3. Word Embedding
Word embeddings were a massive leap forward because they could capture the **semantic meaning** of a word in a dense vector of numbers. Unlike previous methods, these representations understood that words like "king" and "queen" are more related to each other than to "cricketer."

==How Word Embeddings are Created?==
Word embeddings are generated by training a neural network (like Word2Vec or GloVe) on a massive corpus of text, such as all of Wikipedia. The network learns to assign a multi-dimensional vector to each word. The size of this vector is a design choice, often ranging from 64 to 512 dimensions.

For example, in a simplified 3-dimensional space, the vectors might look like this:
- **king**: `[0.9, 0.1, 1.1]`

- **queen**: `[0.9, 0.1, 3.1]`

- **cricketer**: `[0.1, 0.8, 2.4]`

The key insight is that words with similar meanings will have vectors that are close to each other in this high-dimensional space. The "closeness" or similarity between two word vectors can be mathematically measured using **cosine similarity** or the **dot product**.

Furthermore, the individual components (dimensions) of the vector often capture specific attributes or features. In the example above, the first component might represent "royalty," which is why it's high for "king" and "queen" but low for "cricketer." While we don't explicitly define what each dimension means, the network learns these relationships from the data. 

==Problem with word embedding:==
Word embedding are trained once and used multiple times. So the meaning they carry is static. Lets say, we trained our word embedding model on a lot of technology data. 
and our embedding for apple came as
apple: \[0.3, 0.9]
where the first component defines taste and the second component technology.

Now if we want to translate: apple was kept over there with other fruits. 
This will result in wrong results ,interpreting apple as technology.

So what we need is context dependent embedding.

# Self Attention
Self attention solves this problem of static embedding.
What it does it takes all the embedding of the words in that sentence and pass it through the self attention, which provides context dependent embedding based on all other words in the sentence.

[[How Self Attention Works]]




