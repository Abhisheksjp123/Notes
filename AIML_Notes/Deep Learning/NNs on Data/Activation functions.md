Nodes in Neural networks contains various kinds of [[Activation functions]], these curves can be reshaped by the parameter value and then added together to get the green wiggle that fits the data.
![[638866503780322530.png]]
The Current one Used in the plot is Called Softplus

Other popular activation functions
1. SoftPlus:
$$
f(x) = ln(1 + e^x)
$$

![[Pasted image 20250627195416.png]]
2. ReLU (Rectified linear unit)
$$
   f(x) = max(0, x)
$$

![[638866507558657626.png]]
3. Sigmoid function:
$$
   f(x) = 1 / (1 + e^{-x})
$$

![[638866507882963222.png]]

